services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm
    entrypoint: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
    command:
      - "--model"
      - "/models/gpt-oss-20b"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8888"
      - "--tensor-parallel-size"
      - "1"
      - "--gpu-memory-utilization"
      - "0.85"
      - "--dtype"
      - "bfloat16"
      - "--max-model-len"
      - "4096"
      - "--max-num-seqs"
      - "128"
      - "--max-num-batched-tokens"
      - "4096"
      - "--served-model-name"
      - "gpt-oss-20b"
      - "--trust-remote-code"
      - "--enforce-eager"
    runtime: nvidia
    environment:
      TIKTOKEN_RS_CACHE_DIR: "/models/gpt-oss-20b/TIKTOKEN/"
      TIKTOKEN_CACHE_DIR: "/models/gpt-oss-20b/TIKTOKEN/"
      TIKTOKEN_ENCODINGS_BASE: "/models/gpt-oss-20b/TIKTOKEN/"
      TORCH_CUDA_ARCH_LIST: "12.0"
      PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:True"
      TRITON_OVERRIDE_ARCH: "sm120"
      CUDA_VISIBLE_DEVICES: "0"
    ports:
      - "8888:8888"
    volumes:
      - "${HOME}/gpt-oss-20b:/models/gpt-oss-20b"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/v1/models"]
      interval: 10s
      timeout: 5s
      retries: 6
      start_period: 60s

  spectrum_server:
    build:
      context: spectrum_server
      dockerfile: Dockerfile
    image: spectrum_server:latest
    container_name: spectrum_server
    network_mode: "host"
    # Note: Using specific device access instead of privileged mode for security
    # privileged: true  # REMOVED - use device mapping below instead
    command: ["python", "-m", "spectrum_server"]
    environment:
      HOST_NAME: "${HOST_NAME:-localhost}"
      LLM_API: "${LLM_API:-http://localhost:8888/v1}"
      LLM_MODEL: "${LLM_MODEL:-gpt-oss-20b}"
      LLM_API_KEY: "${LLM_API_KEY:?LLM_API_KEY must be set}"
      REASONING_LEVEL: "${REASONING_LEVEL:-low}"
      SDR_DEVICE_SERIAL: "${SDR_DEVICE_SERIAL:-}"
      SDR_BACKEND: "${SDR_BACKEND:-uhd}"
      SYSTEM_PROMPT: "${SYSTEM_PROMPT:-You are an expert spectrum analyzer scientific measurement assistant. You can tune a receiver to different parts of the spectrum and detect intermittent or continuous energy. You can determine the RSSI measurement of the energy as well as the spectrogram_diff score of the overall captured bandwidth.}"
    devices:
      - /dev/bus/usb:/dev/bus/usb
    volumes:
      - /usr/share/uhd:/usr/share/uhd:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 6
      start_period: 60s
    depends_on:
      vllm:
        condition: service_healthy

  agent:
    build:
      context: agent
      dockerfile: Dockerfile
    image: agent:latest
    container_name: agent
    command: ["--server"]
    ports:
      - 8001:8001
    environment:
      SPECTRUM_SERVER_MCP_SERVER: "${SPECTRUM_SERVER_MCP_SERVER:-http://host.docker.internal:8000}"
      SPECTRUM_SERVER_MCP_ROUTE: "${SPECTRUM_SERVER_MCP_ROUTE:-/llm/mcp/}"
      LLM_API_KEY: "${LLM_API_KEY:?LLM_API_KEY must be set}"
      HOST_NAME: "${HOST_NAME:-host.docker.internal}"
      LLM_API: "${LLM_API:-http://vllm:8888/v1}"
      LLM_MODEL: "${LLM_MODEL:-gpt-oss-20b}"
      REASONING_LEVEL: "${REASONING_LEVEL:-low}"
      SYSTEM_PROMPT: "${SYSTEM_PROMPT:-You are an expert spectrum analyzer scientific measurement assistant. You can tune a receiver to different parts of the spectrum and detect intermittent or continuous energy. You can determine the RSSI measurement of the energy as well as the spectrogram_diff score of the overall captured bandwidth.}"
    volumes:
      - /etc/ssl/certs/ca-certificates.crt:/etc/ssl/certs/ca-certificates.crt:ro
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      vllm:
        condition: service_healthy
      spectrum_server:
        condition: service_healthy

  mongodb:
    image: mongo:8.0
    container_name: mongodb
    ports:
      - 27018:27018
    command: mongod --port 27018
